{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B,T,C = 4,8,2\n",
    "x_b_t_c = torch.randn(B,T,C)\n",
    "x_b_t_c.shape\n",
    "\n",
    "xbow_b_t_c = torch.zeros(B,T,C) ## bow for bag of words, which means an average of tokens \n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev_b_t_c = x_b_t_c[b,:t+1,:]\n",
    "        xbow_b_t_c[b,t,:] = torch.mean(xprev_b_t_c,dim=0)\n",
    "\n",
    "x_b_t_c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "xbow_b_t_c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## compute xbow optimized \n",
    "def compute_xbow_opt(x_b_t_c):\n",
    "    B,T,C = x_b_t_c.shape\n",
    "    triu_upper_t_c = torch.triu(torch.ones(T, T), diagonal=0)\n",
    "#    print(triu_upper_t_c.shape)\n",
    "#    print(triu_upper_t_c.sum(dim=0,keepdim=True).shape)\n",
    "#    print(triu_upper_t_c.sum(dim=1,keepdim=True).shape)\n",
    "    \n",
    "    xbow_b_t_c = torch.zeros(B,T,C) ## bow for bag of words, which means an average of tokens \n",
    "\n",
    "    for c in range(C):\n",
    "        xbow_b_t_c[:,:,c] = x_b_t_c[:,:,c] @ triu_upper_t_c\n",
    "#        xbow_b_t_c[:,:,c] /= torch.arange(1,T+1).view(1,-1)\n",
    "        xbow_b_t_c[:,:,c] /= triu_upper_t_c.sum(dim=0, keepdim=True)\n",
    "    return xbow_b_t_c\n",
    "\n",
    "def compute_xbow_opt_v1(x_b_t_c):\n",
    "    B,T,C = x_b_t_c.shape\n",
    "    triu_down_t_c = torch.tril(torch.ones(T, T), diagonal=0)\n",
    "    triu_down_t_c /= triu_down_t_c.sum(dim=1, keepdim=True)        \n",
    "    return triu_down_t_c @ x_b_t_c\n",
    "\n",
    "xbow_b_t_c = compute_xbow_opt(x_b_t_c)\n",
    "print(xbow_b_t_c[0])\n",
    "\n",
    "xbow_b_t_c_1 = compute_xbow_opt_v1(x_b_t_c)\n",
    "print(xbow_b_t_c[0])\n",
    "\n",
    "torch.allclose(xbow_b_t_c, xbow_b_t_c_1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # B x T x head_size\n",
    "q = query(x) # B x T x head_size\n",
    "\n",
    "wei  =q @ k.transpose(-2,-1)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T), diagonal=0)\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = torch.softmax(wei, dim=-1)\n",
    "print(wei[0])\n",
    "v = value(x)\n",
    "out = wei @ v "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
