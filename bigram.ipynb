{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars:  65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "cc \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Prepare our data\n",
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text[:30]\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print(\"total chars: \", len(chars))\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "### Encoder and decoder \n",
    "char2id = {c: i for i, c in enumerate(chars)}\n",
    "id2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "encode = lambda x: [char2id[c] for c in x]\n",
    "decode = lambda x: ''.join([id2char[i] for i in x])\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size:  1003854\n",
      "[18, 47, 56, 57, 58, 1, 15, 47, 58]\n",
      "val data size:  111540\n",
      "Given context [24] the next char [43]\n",
      "Given context [24, 43] the next char [58]\n",
      "Given context [24, 43, 58] the next char [5]\n",
      "Given context [24, 43, 58, 5] the next char [57]\n",
      "Given context [24, 43, 58, 5, 57] the next char [1]\n",
      "Given context [24, 43, 58, 5, 57, 1] the next char [46]\n",
      "Given context [24, 43, 58, 5, 57, 1, 46] the next char [43]\n",
      "Given context [24, 43, 58, 5, 57, 1, 46, 43] the next char [39]\n",
      "Given context [44] the next char [53]\n",
      "Given context [44, 53] the next char [56]\n",
      "Given context [44, 53, 56] the next char [1]\n",
      "Given context [44, 53, 56, 1] the next char [58]\n",
      "Given context [44, 53, 56, 1, 58] the next char [46]\n",
      "Given context [44, 53, 56, 1, 58, 46] the next char [39]\n",
      "Given context [44, 53, 56, 1, 58, 46, 39] the next char [58]\n",
      "Given context [44, 53, 56, 1, 58, 46, 39, 58] the next char [1]\n"
     ]
    }
   ],
   "source": [
    "### Now prepare our training set\n",
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "train_ratio = 0.9\n",
    "\n",
    "data = encode(text)\n",
    "train_data = data[:int(len(text)*train_ratio)]\n",
    "val_data   = data[int(len(text)*train_ratio):]\n",
    "\n",
    "print(\"train data size: \", len(train_data))\n",
    "print(train_data[:block_size+1])\n",
    "print(\"val data size: \", len(val_data))\n",
    "\n",
    "def get_batch(data):\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(data[i:i+block_size],dtype = torch.long ) for i in ix])\n",
    "    y = torch.stack([torch.tensor(data[i+1:i+block_size+1], dtype = torch.long) for i in ix])    \n",
    "    return x,y\n",
    "\n",
    "x_b_l,y_b_1 = get_batch(train_data)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(block_size):\n",
    "        print(\"Given context {} the next char {}\".format(x_b_l[i,:j+1].tolist(), [y_b_1[i,j:j+1].item()]))\n",
    "#        print(\"Given context {} the next char {}\".format(decode(x[i,:j+1].tolist()), decode([y[i,j:j+1].item()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "Generated text:  \n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "## Bigram\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    ## b: batch\n",
    "    ## l: context length\n",
    "    ## c: channel of the output\n",
    "    def forward(self, input_b_l, target_b_1): \n",
    "        out_b_l_c = self.embedding_table(input_b_l)\n",
    "        loss = None\n",
    "\n",
    "        B,L,C = out_b_l_c.shape\n",
    "\n",
    "        if target_b_1 is not None:\n",
    "            target_b_1 = target_b_1.view(B * L)\n",
    "            out_b_l_c = out_b_l_c.view(B*L,C)\n",
    "            loss = F.cross_entropy(out_b_l_c, target_b_1)\n",
    "\n",
    "        return out_b_l_c, loss\n",
    "    \n",
    "    def generate(self, x_b_l, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits_bl_c, _ = self.forward(x_b_l, None)\n",
    "            logits_b_c = logits_bl_c[:, -1, :]\n",
    "            probs_b_c = F.softmax(logits_b_c, dim=1)\n",
    "            idx_next_b_1 = torch.multinomial(probs_b_c, num_samples=1)\n",
    "            x_b_l = torch.cat([x_b_l, idx_next_b_1], dim=1)\n",
    "            \n",
    "        return x_b_l\n",
    "    \n",
    "model = BigramModel(len(chars))\n",
    "out_bl_c, loss = model(x_b_l, y_b_1) \n",
    "print(out_bl_c.shape, loss)\n",
    "\n",
    "started_text_1_1 = torch.zeros(1,1, dtype=torch.long)\n",
    "g_text = model.generate(started_text_1_1, max_new_tokens=100)[0].tolist()\n",
    "print(\"Generated text: \", decode(g_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.629197120666504\n",
      "3.578076124191284\n",
      "3.446087598800659\n",
      "3.5076892375946045\n",
      "3.3700804710388184\n",
      "3.20782208442688\n",
      "3.1579647064208984\n",
      "3.18367075920105\n",
      "3.142939805984497\n",
      "3.0863029956817627\n",
      "3.0159080028533936\n",
      "3.0810744762420654\n",
      "3.002354860305786\n",
      "2.9118049144744873\n",
      "2.8697428703308105\n",
      "2.8411991596221924\n",
      "2.833984375\n",
      "2.7227883338928223\n",
      "2.7264411449432373\n",
      "2.7733335494995117\n",
      "2.767742395401001\n",
      "2.7809245586395264\n",
      "2.7070724964141846\n",
      "2.7179293632507324\n",
      "2.677147626876831\n",
      "2.6648378372192383\n",
      "2.717682123184204\n",
      "2.807802200317383\n",
      "2.5662364959716797\n",
      "2.636556386947632\n",
      "2.6038124561309814\n",
      "2.5512754917144775\n",
      "2.5618717670440674\n",
      "2.6359477043151855\n",
      "2.516904830932617\n",
      "2.532054901123047\n",
      "2.5444071292877197\n",
      "2.5594143867492676\n",
      "2.586312770843506\n",
      "2.5949020385742188\n",
      "2.5698494911193848\n",
      "2.5601165294647217\n",
      "2.6076669692993164\n",
      "2.527657985687256\n",
      "2.5491535663604736\n",
      "2.536827564239502\n",
      "2.5520572662353516\n",
      "2.552196502685547\n",
      "2.5812461376190186\n",
      "2.5536060333251953\n",
      "2.5996265411376953\n",
      "2.4350359439849854\n",
      "2.416480541229248\n",
      "2.480340003967285\n",
      "2.3880228996276855\n",
      "2.618116855621338\n",
      "2.3979125022888184\n",
      "2.5033118724823\n",
      "2.5347816944122314\n",
      "2.578122138977051\n",
      "2.523906946182251\n",
      "2.4899675846099854\n",
      "2.401623487472534\n",
      "2.480825424194336\n",
      "2.4427855014801025\n",
      "2.480722665786743\n",
      "2.4852023124694824\n",
      "2.505082368850708\n",
      "2.5463991165161133\n",
      "2.4273085594177246\n",
      "2.5840647220611572\n",
      "2.5306379795074463\n",
      "2.507204532623291\n",
      "2.4818317890167236\n",
      "2.6353797912597656\n",
      "2.4534878730773926\n",
      "2.482269048690796\n",
      "2.4326677322387695\n",
      "2.5710620880126953\n",
      "2.398608446121216\n",
      "2.535327672958374\n",
      "2.4593863487243652\n",
      "2.3877830505371094\n",
      "2.3356411457061768\n",
      "2.466754674911499\n",
      "2.396059274673462\n",
      "2.506127119064331\n",
      "2.4401001930236816\n",
      "2.42234206199646\n",
      "2.278822422027588\n",
      "2.4217193126678467\n",
      "2.5465829372406006\n",
      "2.4090354442596436\n",
      "2.4790279865264893\n",
      "2.4138529300689697\n",
      "2.4748456478118896\n",
      "2.4204494953155518\n",
      "2.362253427505493\n",
      "2.488394021987915\n",
      "2.472989320755005\n"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb,yb = get_batch(train_data)\n",
    "    \n",
    "    logits,loss = model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()    \n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "\n",
      "Codaketwerothait g ESoco pe teran Gly? go!\n",
      "Slirenkneverajest young y, cet l th e?\n",
      "I he t ho jusofas my wd the y, ard h fes, her, KE: e,\n",
      "\n",
      "Heas tiluroullowh the as hy ea if ansountante dveaghed shavir-laryould s tulintherit?\n",
      "intus gimbes ad y meroprexco ghthimee, ie, grd!\n",
      "A:\n",
      "\n",
      "Mofol yowind thoushenod t.\n",
      "ARis bey\n",
      "Whes RDOLENThed min y VOren thethe tll; ive dse mye\n",
      "CHAus\n",
      "Tratoreithanghis taifthinereys\n"
     ]
    }
   ],
   "source": [
    "started_text_1_1 = torch.zeros(1,1, dtype=torch.long)\n",
    "g_text = model.generate(started_text_1_1, max_new_tokens=400)[0].tolist()\n",
    "print(\"Generated text: \", decode(g_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
