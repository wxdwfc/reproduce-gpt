{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars:  65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "### Prepare our data\n",
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text[:30]\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print(\"total chars: \", len(chars))\n",
    "print(''.join(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "### Encoder and decoder \n",
    "char2id = {c: i for i, c in enumerate(chars)}\n",
    "id2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "encode = lambda x: [char2id[c] for c in x]\n",
    "decode = lambda x: ''.join([id2char[i] for i in x])\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check CUDA available:  True\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'encode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      9\u001b[0m train_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n\u001b[0;32m---> 11\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mencode\u001b[49m(text)\n\u001b[1;32m     12\u001b[0m train_data \u001b[38;5;241m=\u001b[39m data[:\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;241m*\u001b[39mtrain_ratio)]\n\u001b[1;32m     13\u001b[0m val_data   \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;241m*\u001b[39mtrain_ratio):]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encode' is not defined"
     ]
    }
   ],
   "source": [
    "### Now prepare our training set\n",
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "print(\"Check CUDA available: \", torch.cuda.is_available())\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "train_ratio = 0.9\n",
    "\n",
    "data = encode(text)\n",
    "train_data = data[:int(len(text)*train_ratio)]\n",
    "val_data   = data[int(len(text)*train_ratio):]\n",
    "\n",
    "print(\"train data size: \", len(train_data))\n",
    "print(train_data[:block_size+1])\n",
    "print(\"val data size: \", len(val_data))\n",
    "\n",
    "def get_batch(data):\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(data[i:i+block_size],dtype = torch.long ) for i in ix])\n",
    "    y = torch.stack([torch.tensor(data[i+1:i+block_size+1], dtype = torch.long) for i in ix])    \n",
    "    return x,y\n",
    "\n",
    "x_b_l,y_b_1 = get_batch(train_data)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(block_size):\n",
    "        print(\"Given context {} the next char {}\".format(x_b_l[i,:j+1].tolist(), [y_b_1[i,j:j+1].item()]))\n",
    "#        print(\"Given context {} the next char {}\".format(decode(x[i,:j+1].tolist()), decode([y[i,j:j+1].item()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "Generated text:  \n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "## Bigram\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    ## b: batch\n",
    "    ## l: context length\n",
    "    ## c: channel of the output\n",
    "    def forward(self, input_b_l, target_b_1): \n",
    "        out_b_l_c = self.embedding_table(input_b_l)\n",
    "        loss = None\n",
    "\n",
    "        B,L,C = out_b_l_c.shape\n",
    "\n",
    "        if target_b_1 is not None:\n",
    "            target_b_1 = target_b_1.view(B * L)\n",
    "            out_b_l_c = out_b_l_c.view(B*L,C)\n",
    "            loss = F.cross_entropy(out_b_l_c, target_b_1)\n",
    "\n",
    "        return out_b_l_c, loss\n",
    "    \n",
    "    def generate(self, x_b_l, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits_bl_c, _ = self.forward(x_b_l, None)\n",
    "            logits_b_c = logits_bl_c[:, -1, :]\n",
    "            probs_b_c = F.softmax(logits_b_c, dim=1)\n",
    "            idx_next_b_1 = torch.multinomial(probs_b_c, num_samples=1)\n",
    "            x_b_l = torch.cat([x_b_l, idx_next_b_1], dim=1)\n",
    "            \n",
    "        return x_b_l\n",
    "    \n",
    "model = BigramModel(len(chars))\n",
    "out_bl_c, loss = model(x_b_l, y_b_1) \n",
    "print(out_bl_c.shape, loss)\n",
    "\n",
    "started_text_1_1 = torch.zeros(1,1, dtype=torch.long)\n",
    "g_text = model.generate(started_text_1_1, max_new_tokens=100)[0].tolist()\n",
    "print(\"Generated text: \", decode(g_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.704006195068359\n",
      "4.659500598907471\n",
      "4.471990585327148\n",
      "4.323152542114258\n",
      "4.255801677703857\n",
      "4.245046138763428\n",
      "4.165693759918213\n",
      "4.048964977264404\n",
      "4.097479343414307\n",
      "3.7496376037597656\n",
      "3.7070794105529785\n",
      "3.716240406036377\n",
      "3.637645959854126\n",
      "3.424874782562256\n",
      "3.433396577835083\n",
      "3.427090644836426\n",
      "3.3038835525512695\n",
      "3.2864811420440674\n",
      "3.190141439437866\n",
      "3.202833414077759\n",
      "3.139291763305664\n",
      "3.0029618740081787\n",
      "3.0597994327545166\n",
      "2.9590420722961426\n",
      "2.982276201248169\n",
      "2.9200470447540283\n",
      "2.84088134765625\n",
      "2.8899765014648438\n",
      "2.9750688076019287\n",
      "2.808044672012329\n",
      "2.7770206928253174\n",
      "2.747230291366577\n",
      "2.6850526332855225\n",
      "2.679885149002075\n",
      "2.68688702583313\n",
      "2.810159683227539\n",
      "2.691971778869629\n",
      "2.66461181640625\n",
      "2.6310133934020996\n",
      "2.7520360946655273\n",
      "2.5809037685394287\n",
      "2.629011869430542\n",
      "2.624750852584839\n",
      "2.547957181930542\n",
      "2.58158540725708\n",
      "2.6034939289093018\n",
      "2.617574453353882\n",
      "2.5722484588623047\n",
      "2.511366367340088\n",
      "2.6074514389038086\n",
      "2.5077037811279297\n",
      "2.5723509788513184\n",
      "2.4938509464263916\n",
      "2.5230987071990967\n",
      "2.4825401306152344\n",
      "2.545375347137451\n",
      "2.5758450031280518\n",
      "2.600005626678467\n",
      "2.3530614376068115\n",
      "2.4447317123413086\n",
      "2.528287172317505\n",
      "2.4946839809417725\n",
      "2.4428963661193848\n",
      "2.4436542987823486\n",
      "2.5285544395446777\n",
      "2.468841075897217\n",
      "2.5544025897979736\n",
      "2.46596360206604\n",
      "2.6043810844421387\n",
      "2.56870698928833\n",
      "2.501732587814331\n",
      "2.440998077392578\n",
      "2.520970344543457\n",
      "2.4477615356445312\n",
      "2.406963348388672\n",
      "2.395866870880127\n",
      "2.4263803958892822\n",
      "2.4208271503448486\n",
      "2.491838216781616\n",
      "2.4361510276794434\n",
      "2.466592311859131\n",
      "2.49214506149292\n",
      "2.405332565307617\n",
      "2.6104485988616943\n",
      "2.4645237922668457\n",
      "2.4175832271575928\n",
      "2.4606645107269287\n",
      "2.440732479095459\n",
      "2.635645627975464\n",
      "2.3335728645324707\n",
      "2.4820709228515625\n",
      "2.532583236694336\n",
      "2.5850303173065186\n",
      "2.370664358139038\n",
      "2.443483829498291\n",
      "2.4058077335357666\n",
      "2.525002956390381\n",
      "2.5208353996276855\n",
      "2.4349474906921387\n",
      "2.4284467697143555\n"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb,yb = get_batch(train_data)\n",
    "    \n",
    "    logits,loss = model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()    \n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercckehathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThineent.\n",
      "\n",
      "Lavinde.\n",
      "athave l.\n",
      "KEONGBUCHandspo be y,-hedarwnoddy scace, tridesar, wne'shenou\n"
     ]
    }
   ],
   "source": [
    "started_text_1_1 = torch.zeros(1,1, dtype=torch.long)\n",
    "g_text = model.generate(started_text_1_1, max_new_tokens=400)[0].tolist()\n",
    "print(\"Generated text: \", decode(g_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
